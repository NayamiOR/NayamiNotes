# 第二章 支持向量机

# 线性可分定义

## 线性可分

### 用数学严格定义线性可分

直线方程：$w_1x_1+w_2x_2+b=0$

其中：

权重：$w_1,w_2$

偏置：$b$

在特征空间中，分割直线的两侧，一侧$w_1x_1+w_2x_2+b>0$，一侧$w_1x_1+w_2x_2+b<0$

假设有N个训练样本和标签

$$\{(X_1,y_1),(X_2,y_2),...,(X_N,y_N\}$$

其中$X_i=[x_{i_1},x_{i_2}]^T$是一个向量

$y_i=\{+1,-1\}$是$X_i$的标签，如果$X_i$属于$C_1$，那么$y_i$是+1，如果$X_i$属于$C_2$，那么$y_i$是-1

### 用向量形式定义线性可分

![%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled.png](%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled.png)

### 最简化形式

![%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled%201.png](%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled%201.png)

# 问题描述

## 寻找最优分类直线

**基于最优化理论**

假设

对任意一条分开两类的直线，将直线朝一侧平移，直到它碰到一个及以上训练样本，同时朝另一侧进行相同的操作。定义平移后直线碰到的训练样本为支持向量，定义向两侧平移后的直线距离为间隔

**支持向量机寻找的最优分类直线应该满足**

1. 分开了两类
2. 间隔最大化
3. 直线位于间隔的中间，到所有支持向量距离相等

以上基于二维特征空间的结果

# 优化问题

最优分类超平面应该满足：

1. 该超平面分开了两类
2. 该超平面有最大化间隔
3. 该超平面处于间隔的中间，到所有支持向量距离相等

支持向量机需要找的是**最大化间隔**的超平面

优化问题可以写成如下形式：

最小化：$\frac{1}{2}||w||^2$，也就是最小化omega模的平方

限制条件：$y_i(w^Tx_i+b)\geq1,(i=1\sim N)$

事实1：

$\omega ^Tx+b=0$与

$(a\omega^T)x+(ab)=0$是同一个超平面$(a\neq0)$

事实2：

一个点$X_0$到超平面$\omega^Tx_0+b=0$的距离

$d=\frac{|\omega^Tx_0+b|}{||\omega||}$

二维平面中事实2的特例：

![%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled%202.png](%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled%202.png)

**优化问题推到中最难理解的部分：**

用$a$去缩放$\omega b$，使$(\omega,b)\rightarrow (a\omega,ab)$（缩放前后表示的是同一个平面）

最终使在支持向量$x_0$上有$|\omega^Tx_0+b|=1$

而在非支持向量上$|\omega^Tx_0+b|>1$

$\because$$d=\frac{|\omega^Tx_0+b|}{||\omega||}$

$\therefore$$d=\frac{1}{||\omega||}$

那么最大化支持向量到超平面的距离就是最小化omega的模

以上问题中$(X_i,y_i),i=1\sim N$是已知的，$(\omega,b)$是待求的。

这是凸优化问题中的二次规划问题

二次规划的定义：

1. 目标函数是二次项
2. 限制条件是一次项

如果一个问题是凸优化问题，那么当成是一个已经解决了的问题，因为它有唯一的一个全局极值

线性可分条件下的支持向量机求解是一个凸优化问题，因此可以被快速解决

# 线性不可分

在线性不可分的情况当中，以上的优化问题无解，此时求不出$(\omega,b)$所以要放松限制条件

基本思路是对每一个样本及标签$(X_i,y_i)$设置一个松弛变量$\delta_i$

于是限制条件可以改写为：$y_i(\omega^TX_i+b)\geq1-\delta_i(i=1\sim N)$

然后应该加入新的限制

## 改造后的支持向量机优化版本

最小化：$\frac{1}{2}||\omega ||^2+C\sum_{i=1}^{N}{\delta_i}$或$\frac{1}{2}||\omega ||^2+C\sum_{i=1}^{N}\delta_i^2$

限制条件：

1. $\delta_i \geq0,(i=1\sim N)$
2. $y_i(\omega^TX_i+b)\geq1-\delta_i,(i=1\sim N)$

以前的目标函数只需要最小化$\frac{1}{2}||\omega||^2$

现在的目标函数同时还需要最小化所有$\delta_i$的和

以上公式中，平衡因子C是需要人为设定的

人为事先设定的参数叫做**算法的超参数**

在实际应用中要不断变化C的值，并同时测试算法的识别率，然后选取使识别率达到最大的超参数C

算法的超参数越多，需要手动调整的地方就越多

支持向量机是超参数很少的算法模型

# 低维到高维的映射

## 扩大可选函数范围

其它算法，如人工神经网络中，采用直接产生更多可选函数的方法，例如通过多层非线性函数的组合产生类似于椭圆的曲线从而区分图中的两类

![%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled%203.png](%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled%203.png)

相比之下，支持向量机会通过将特征空间从低维映射到高维后用线性超平面对数据进行分类的方式

### 问题：

解决方法：

构建一个二维到五维的映射：$\phi(x)$

在五维中，这个例子将有可能变成线性可分

例如：

$\phi(x):x=\begin{bmatrix}a\\b\end{bmatrix}\rightarrow \phi(x)=\begin{bmatrix}a^2\\b^2\\a\\b\\ab\end{bmatrix}$

那么用这种方法就可以算出$x_1,x_2,x_3,x_4$对应的五维分量

![%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled%204.png](%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%204c9cf6501e6a4754bdcb79cab445a23b/Untitled%204.png)

设$\omega=\begin{bmatrix}-1\\-1\\-1\\-1\\6\end{bmatrix}$$b=1$，可以得到$\omega^T\phi(x_1)+b=1\geq0$，$\omega^T\phi(x_2)+b=3\geq0$，$\omega^T\phi(x_3)+b=-1<0$，$\omega^T\phi(x_4)+b=-1<0$

可以看到上述例子在从二维到五维的映射后变成了线性可分

## 说明：

假设：在一个M维空间上随机取N个训练样本随机的对每个训练样本赋予标签-1和+1

假设：这些训练样本线性可分的概率为P(M）

当M趋于无穷大时，P（M）=1

当我们增加特征空间维度的时候，待估计参数$(\omega,b)$的维度也会增加，整个算法模型的自由度会增加，就更有可能分开低维时无法分开的数据集

**将训练样本由低维映射到高维可以增大线性可分的概率**

## 支持向量机优化问题

如果$x$映射为$\phi (x)$，那么前面的限制条件第二条只需要改成：

$y_i[\omega^T\phi(X_i)+b]\geq1=\delta_i,(i=1\sim N)$

# 核函数的定义

不用知道$\phi (x)$的具体形式

$K(X_1,X_2)=\phi(X_1)^T\phi (X_2)$

## 例子：

假设$\phi(X)$是一个将二维向量映射为三维向量的映射

$X=[x_1,x_2]^T$

$\phi(X)=\phi([x_1,x_2]^T)=[x_1^2,x_1x_2,x_2^2]$

假设有两个二维向量

$X_1=[x_{11},x_{12}]^T$

$X_2=[x_{21},x_{22}]^T$

$$\begin{align*}K(X_1,X_2)&=\phi(X_1)^T\phi(X_2)\\&=[x^2_{11},x_{11}x_{12},x^2_{12}][x^2_{21},x_{21}x_{22},x^2_{22}]^T\\&=x^2_{11}x^2_{21}+x_{11}x_{12}x_{21}x_{22}+x_{12}^2x_{22}^2  \end{align*}$$

## 已知核函数K求映射$\phi$的例子

假设：

$$\begin{align*}K(X_1,X_2)&=(x_{11}x_{21}+x_{12}x_{22}+1)^2\\&=x^2_{11}x^2_{21}+x^2_{12}x_{22}^2+1+2x_{11}x_{21}x_{12}x_{22}+2x_{11}x_{21}+2x_{12}x_{22}\\&=\phi(X_1)^T\phi(X^2)\end{align*}$$

**所以核函数K和映射$\phi$是一一对应的关系**

核函数的形式不能随意取，需要满足一定条件，然后才能分解成两个$\phi$内积的形式

## Marcer定理：核函数能写成以上形式的必要条件

1. $K(X_1,X_2)=K(X_2,X_1)$（交换性）
2. $∀C_i(i=1\sim N）$，$∀N有\sum^N_{i=1}\sum^N_{j=1} C_iC_jK(X_i,X_j)\geq0$（半正定性）

只要满足以上条件，就一定能写成Phi内积的形式

# 原问题和对偶问题

## 原问题：

最小化：

$f(\omega)$

限制条件：

$g_i(\omega)\leq0$   ，$i=1\sim K$

$h_i(\omega)=0$，$i=1\sim m$

## 对偶问题：

定义该原问题的对偶问题如下

$$\begin{align*}L(\omega,\alpha,\beta)&=\sum^K_{i=1}a_ig_i(\omega)+\sum^K_{i=1}\beta_ih_i(\omega)\\&=f(\omega)+\alpha^Tg(\omega)+\beta^Th(\omega)\end{align*}$$

以上第二行是写作向量的形式

其中

$$\alpha=[\alpha_1,\alpha_2,\ldots,\alpha_K]^T\\\beta=[\beta_i,\beta_2,\ldots,\beta_K]^T\\g(\omega)=[g_1(\omega),g_2(\omega),,\ldots,g_K(\omega),]^T\\h(\omega)=[h_1(\omega),h_2(\omega),,\ldots,h_M(\omega),]^T$$

在定义了函数$L(\omega,\alpha,\beta)$的基础上，对偶问题如下

最大化：$\theta(\alpha,\beta)=inf L(\omega,\alpha,\beta)$，所有定义域内的$\omega$

限制条件：$\alpha_i\geq0,i=1\sim K$

## 综合原问题和对偶问题的定义可以得到：

定理一：如果$\omega^*$是原问题的解，$(\alpha^*,\beta^*)$是对偶问题的解则有：$f(\omega^*)\geq\theta(\alpha^*,\beta^*)$

$$\text { 证明: } \begin{aligned}\theta\left(\alpha^{*}, \beta^{*}\right) &=\inf L\left(\omega, \alpha^{*}, \beta^{*}\right) \\& \leq L\left(w^{*}, \alpha^{*}, \beta^{*}\right) \\&=f\left(\omega^{*}\right)+\alpha^{* T} g\left(\omega^{*}\right)+\beta^{* T} h\left(\omega^{*}\right) \\& \leq f\left(\omega^{*}\right)\end{aligned}$$

说明原问题的解永远大于等于对偶问题的解

原问题的解-对偶问题的解=对偶差距

对偶差距是一个大于等于0的函数

## 强对偶定理

如果$g(\omega)=\mathbf{A} \omega+\mathbf{b}, \quad \mathbf{h}(\omega)=\mathbf{C} \omega+\mathbf{d} \text { , }$$f(\omega)$为凸函数，则有$f\left(\omega^{*}\right)=\theta\left(\alpha^{*}, \beta^{*}\right)$，则对偶差距为0
# 规则

# Requests库入门

[七个主要方法](%E8%A7%84%E5%88%99%20b7521e8e2dd345c28f1ba80f633a101c/%E4%B8%83%E4%B8%AA%E4%B8%BB%E8%A6%81%E6%96%B9%E6%B3%95%205619713be47a4ba293b59575e66673e9.csv)

## get方法

```python
requests.get(url,params=None,**kwargs)
```

url：页面的url链接

params：url中的额外参数，字典或字节流格式，可选

**kwargs：12个控制访问的参数

![%E8%A7%84%E5%88%99%20b7521e8e2dd345c28f1ba80f633a101c/Untitled.png](%E8%A7%84%E5%88%99%20b7521e8e2dd345c28f1ba80f633a101c/Untitled.png)

Response对象包含爬虫返回的内容

[Response对象的五个属性](%E8%A7%84%E5%88%99%20b7521e8e2dd345c28f1ba80f633a101c/Response%E5%AF%B9%E8%B1%A1%E7%9A%84%E4%BA%94%E4%B8%AA%E5%B1%9E%E6%80%A7%20797f63c0f7eb4a4581b55e2ded46c8fa.csv)

## 爬取网页的通用代码框架

[Requests库的异常](%E8%A7%84%E5%88%99%20b7521e8e2dd345c28f1ba80f633a101c/Requests%E5%BA%93%E7%9A%84%E5%BC%82%E5%B8%B8%20f6cbe811498f4006987d52453035bea2.csv)

## HTTP协议

HTTP协议：超文本传输协议，基于请求与相应模式，无状态的应用层协议

一般采用URL作为定位网络资源的标识

URL格式：http://host[:port][path]

host：合法的Internet主机域名或IP地址

port：端口号，缺省端口为80

path：请求资源的路径 

[HTTP协议对资源的操作](%E8%A7%84%E5%88%99%20b7521e8e2dd345c28f1ba80f633a101c/HTTP%E5%8D%8F%E8%AE%AE%E5%AF%B9%E8%B5%84%E6%BA%90%E7%9A%84%E6%93%8D%E4%BD%9C%202b2e24a886624bbab694196f8022f675.csv)

## Requests库主要方法解析

### request方法

```python
requests.request(method,url,**kwargs)
```

1. 请求方式
2. url链接
3. 十三个控制访问参数

### 控制访问参数

- params：字典或者字节序列作为参数增加到url中
- data：字典，字节序列或者文件对象作为Request的内容
- json：json格式的数据，作为Request的内容
- headers：字典，HTTP定制头（可以用来模拟浏览器）
- cookies：字典或CookieJar，Request的cookie
- auth：元组，支持HTTP认证功能
- files：字典类型，传输文件
- timeout：设定超时时间（秒为单位）
- proxies：字典类型，设定访问代理服务器，可增加登录认证
- allow_redirects：默认True，重定向开关
- stream：默认True，获取内容立即下载开关
- verify：默认True，认证SSL证书开关
- cert：本地SSL证书路径

### 其它六个方法

```python
requests.get(url,params=None,**kwargs)
requests.head(url,**kwargs)
requests.post(url,data=None,json=None,**kwargs)
requests.patch(url,data=None,**kwargs)
requests.delete(url,**kwargs)
```

## Robots协议

**网络爬虫的尺寸**

小规模，量小，速度不敏感：Requests库

中规模，量大，速度敏感：Scrapy库

大规模，搜索引擎等：定制开发

**引发问题**

- 骚扰问题
- 法律风险
- 隐私泄露

### 网络爬虫的限制

- 来源审查：判断User-Agent
    - 只相应浏览器和友好爬虫的访问
- 发布公告：Robots协议
    - 告知爬取策略，要求爬虫遵守

Robots Exclusion Standard：网络爬虫排除标准

作用：网站告知网络爬虫哪些页面可以抓取，哪些不行

形式：网站根目录下的robots.txt文件

文件中：

User-Agent表明的是哪些爬虫，代表所有就用*

Disallow代表不允许爬虫访问的资源目录

### Robots协议的使用

网络爬虫：自动或人工识别

 建议，非约束性，可以不遵守但是存在法律风险

类人行为可不参考robots协议